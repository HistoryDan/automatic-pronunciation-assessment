{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6WvmnlV0QL3"
   },
   "source": [
    "After labeling the pronunciation scores of the speech data, we performed data augmentation. There are many benefits of it, but here are some of the most important ones.\n",
    "\n",
    "1. **Improvement in Generalization Ability**: Augmentation helps the model to not overly rely on specific environments or conditions. This enables the model to maintain high performance in various real-world situations.\n",
    "2. **Prevention of Overfitting**: Augmentation aids in preventing overfitting when the training data is limited. Exposure to diverse forms and types of data enhances the generalization ability, preventing the model from fitting to closely to the training set.\n",
    "3. **Creation of Robust Models**: Augmentation helps in making model more robust and resilient. For example, it enhances the model’s ability to handle noise, environmental variations, and imperfect speech, contributing to its robustness in real-world scenarios.\n",
    "\n",
    "The most important part of data augmentation is it can ensure the reliability of the model. Speech recorded by A.I speaker is susceptible to ambient noise. However, by adding noise and other sound effects during the augmentation process, we can create a model that is robust to these situations. To augment wav files, we referred “Data Augmenting Contrastive Learning of Speech Representations in the Time Domain” (Kharitonov et al., 2020) from paperswithcode, and used WavAugment library.\n",
    "\n",
    "[Papers with Code - Data Augmenting Contrastive Learning of Speech Representations in the Time Domain](https://paperswithcode.com/paper/data-augmenting-contrastive-learning-of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Hrk3mn1h819"
   },
   "source": [
    "# 1. Download and import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kWhN83i0TUn"
   },
   "source": [
    "First, install sox (Sound exChange), torchaudio, and WavAugment, and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20318,
     "status": "ok",
     "timestamp": 1701521132125,
     "user": {
      "displayName": "Junhyeong Park",
      "userId": "07560668274037058289"
     },
     "user_tz": -540
    },
    "id": "c2RMC2mFCSrJ",
    "outputId": "3df9756b-2ef6-4110-edcb-2cf8d44fe989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/WavAugment.git /tmp/pip-req-build-6kvn9ns5\n"
     ]
    }
   ],
   "source": [
    "! apt-get install libsox-fmt-all libsox-dev sox > /dev/null\n",
    "! python -m pip install torchaudio > /dev/null\n",
    "! python -m pip install git+https://github.com/facebookresearch/WavAugment.git > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKRk5rr8CQge"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import augment\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18331,
     "status": "ok",
     "timestamp": 1701521182876,
     "user": {
      "displayName": "Junhyeong Park",
      "userId": "07560668274037058289"
     },
     "user_tz": -540
    },
    "id": "UZ_hqd7RBibs",
    "outputId": "36ab32ed-fb8f-46f1-9b62-bb9bf51289f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qDwzl-WiAv9"
   },
   "source": [
    "# 2. Audio augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkCrK8qt0Vv5"
   },
   "source": [
    "First, make sure your GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701521185283,
     "user": {
      "displayName": "Junhyeong Park",
      "userId": "07560668274037058289"
     },
     "user_tz": -540
    },
    "id": "BS9qTewEf-lM",
    "outputId": "f1f2e157-a22c-4b00-dee3-ed9982fd9fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oSVruNp0X_M"
   },
   "source": [
    "And then we load `audio_reference_scored.pkl` file that was created in the before stage. This pickle file is containing file paths of each wav files, tensors and whole pronunciation scores. If you want to skip the before stage, you can simply use `audio_reference_scored.pkl` file in the share folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4WcrbXnCGQn"
   },
   "outputs": [],
   "source": [
    "# initialize new dataframe\n",
    "df = pd.read_pickle('your_own_path/audio_reference_scored.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0epZNnFm0bvZ"
   },
   "source": [
    "Next, we check the input file paths (paths of original wav files) and output file paths (paths of augmented wav files) using os library, and store them to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42MxbDi6Ci5Y"
   },
   "outputs": [],
   "source": [
    "# load reference audio files\n",
    "output_path = 'your_own_path/recordings/augmented'\n",
    "original_path = 'your_own_path/recordings/original'\n",
    "output_paths = [os.path.join(output_path, file) for file in os.listdir(original_path)]\n",
    "df['output_path'] = output_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdDTUSb60eGd"
   },
   "source": [
    "After that, we define a audio_modification function, that will modify the original wav files. This function will randomly apply one of four wav augmentation techniques to the source file.\n",
    "\n",
    "1. **Pitch shift**: Make lower or higher the pitch of the voice. For example, -200 indicates that we’ll go lower by 200 cents of the tone.\n",
    "2. **Reverberation**: Add echo to a sound signal, conveying spatial depth and width to the sound. Each parameter in `reverb()` function stands for reverberance, dumping factor, and room size.\n",
    "3. **Noise** : Applying additive noise. In this case, we used generated uniform noise.\n",
    "4. **Time dropout**: Substituting a brief segment of audio with periods of silence. This method is frequently employed in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQFbXz9VCo9D"
   },
   "outputs": [],
   "source": [
    "# function to modify original audio file\n",
    "# change the pitch, add reverb effect, additive noise, and drop out random section of audio\n",
    "\n",
    "def audio_modification(wave_path, random_pitch_shift, random_room_size):\n",
    "  x, sr = torchaudio.load(wave_path)\n",
    "  r = random.randint(1,5)\n",
    "\n",
    "  if r == 1:\n",
    "    random_pitch_shift_effect = augment.EffectChain().pitch(\"-q\", random_pitch_shift).rate(sr)\n",
    "    y = random_pitch_shift_effect.apply(x, src_info={'rate': sr})\n",
    "  elif r == 2:\n",
    "    random_reverb = augment.EffectChain().reverb(50, 50, random_room_size).channels(1)\n",
    "    y = random_reverb.apply(x, src_info={'rate': sr})\n",
    "  elif r == 3:\n",
    "    noise_generator = lambda: torch.zeros_like(x).uniform_()\n",
    "    y = augment.EffectChain().additive_noise(noise_generator, snr=15).apply(x, src_info={'rate': sr})\n",
    "  else:\n",
    "    y = augment.EffectChain().time_dropout(max_seconds=0.5).apply(x, src_info={'rate': sr})\n",
    "\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XepP_Iwi0g9q"
   },
   "source": [
    "Then we initialize the `random_pitch_shift` and `random_room_size` variables. For `random_room_size`, we set it from 0 to 51, and for `random_pitch_shift`, we set it from -100 to 100 so that it doesn’t pitch too high or low since we’re targeting kids. After that, we call the function and store the tensor form of modified wav file data at the `modified_vector` column of the data frame that we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoIiFSA0FC_t"
   },
   "outputs": [],
   "source": [
    "# set randomized parameters\n",
    "random_pitch_shift = lambda: np.random.randint(-100, +100)\n",
    "random_room_size = lambda: np.random.randint(0, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250763,
     "status": "ok",
     "timestamp": 1701521466956,
     "user": {
      "displayName": "Junhyeong Park",
      "userId": "07560668274037058289"
     },
     "user_tz": -540
    },
    "id": "Sxlnc8HrItQu",
    "outputId": "ee2b0183-3331-4e1d-cc79-70324990ff3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2035/2035 [04:10<00:00,  8.12it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['modified_vector'] = df['file_path'].progress_apply(lambda x: audio_modification(x, random_pitch_shift, random_room_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D71aZtJv0jHy"
   },
   "source": [
    "Finally, to check the augmented result files, we converted the `modified_vectors` into wav files and saved them into the `output_path` of each augmented result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1568232,
     "status": "ok",
     "timestamp": 1701523037783,
     "user": {
      "displayName": "Junhyeong Park",
      "userId": "07560668274037058289"
     },
     "user_tz": -540
    },
    "id": "qQBhOKWAL-hU",
    "outputId": "bcf3cfc7-e0e0-4e40-f6ba-fd0bdbf1b545"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2035/2035 [26:08<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate augmented wav files\n",
    "for i in tqdm(range(len(df))):\n",
    "  output_path = df.loc[i, 'output_path']\n",
    "  y = df.loc[i, 'modified_vector']\n",
    "  torchaudio.save(output_path, y, sample_rate = 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7mEb04BiKrH"
   },
   "source": [
    "# 3. Create a new reference pickle file for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mutXgIp0lm-"
   },
   "source": [
    "We create a new reference file `audio_reference_scored_augmented.pkl` that contains all original and augmented wav files, and their pronunciation scores for later use. At this step, we set the pronunciation score of the augmented wav file to be the same as the score of original wav file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScJARw_JeVdn"
   },
   "outputs": [],
   "source": [
    "# Create new dataframe that contains original and augmented wav file paths and their features\n",
    "file_path = df['file_path'].to_list() + df['output_path'].to_list()\n",
    "accuracy = df['accuracy'].to_list() * 2\n",
    "completeness = df['completeness'].to_list() * 2\n",
    "fluency = df['fluency'].to_list() * 2\n",
    "prosodic = df['prosodic'].to_list() * 2\n",
    "\n",
    "result = pd.DataFrame(columns = ['file_path', 'vector', 'accuracy', 'completeness', 'fluency', 'prosodic'])\n",
    "result['file_path'] = file_path\n",
    "result['accuracy'] = accuracy\n",
    "result['completeness'] = completeness\n",
    "result['fluency'] = fluency\n",
    "result['prosodic'] = prosodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuO2Z4s_fVHd"
   },
   "outputs": [],
   "source": [
    "# save the data frame\n",
    "result.to_pickle('your_own_path/audio_reference_scored_augmented.pkl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMyujfQDMpvLTZPoNLytKfA",
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
